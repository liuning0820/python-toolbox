# python-toolbox

A collection of small, useful tools written in Python.

## Features

- Various handy scripts for networking, IP calculation, and more
- Easy to extend with your own Python utilities
- Dependency management with [uv](https://github.com/astral-sh/uv) for fast and reproducible installs

## Getting Started

### Prerequisites

- Python 3.8+
- [uv](https://github.com/astral-sh/uv) (install via `pip install uv`)

### Generating requirements.txt

You can generate or update `requirements.txt` with your current environment's dependencies using:

```bash
uv pip freeze > requirements.txt
```

### Install dependencies

If you are in China, you can use the Tsinghua mirror for faster access:

```bash
uv pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### Example Tools

- `cidr_calculator.py`: Calculate CIDR notation, network/broadcast address, and IP range for a given IP and subnet mask.
- `get_ip.py`: Fetch your public IP address using an online service.python get_pdf_meta.py -F your_file.pdf
- `get_pdf_meta.py`: Extract metadata from a PDF file, such as title, author, and creation date.

### Usage

Run any tool directly with Python, for example:

```bash
python cidr_calculator.py
python get_ip.py
python get_pdf_meta.py -F your_file.pdf
python conn-ports-scan.py -H <targetHost> -p <targetPort>
streamlit run ollama_chat_with_pdf.py
streamlit run ollama_url_summarizer.py
```

---

## Adding New Tools

1. Write your Python script in this directory.
2. Add any new dependencies to `requirements.txt`.
3. Install them with:

   ```bash
   uv pip install -r requirements.txt
   ```


## Web Page Summarizer with Ollama

You can summarize the content of any web page using a local Ollama LLM with the `streamlit_ollama_url_summarizer.py` tool.

### Setup

1. Make sure you have the required dependencies installed:
   ```bash
   uv pip install -r requirements.txt
   ```

2. Create a `.env` file in the project root (if you want to override defaults):

   ```
   OLLAMA_MODEL=qwen3:32b
   OLLAMA_LLM_BASE_URL=http://localhost:11434
   ```

   - `OLLAMA_MODEL` is the name of the Ollama model to use.
   - `OLLAMA_LLM_BASE_URL` is the base URL for your Ollama server.

### Running the Summarizer

Start the Streamlit app with:

```bash
streamlit run ollama_url_summarizer.py
```

- Paste the URL of the web page you want to summarize into the input box.
- Click "Summarize" to get a summary generated by the selected Ollama model.

---


