# rag_app.py
import streamlit as st
import chromadb
from ollama import embed, chat
from pathlib import Path

from dotenv import load_dotenv
import os


load_dotenv()
DB_DIR = os.environ.get("DB_DIR", "rag_vectors_db")
MODEL_NAME = os.environ.get("MODEL_NAME", "gemma3:1b")
EMBEDDING_MODEL = os.environ.get("OLLAMA_EMBEDDING_MODEL", "bge-m3")


client = chromadb.PersistentClient(path=DB_DIR)

# å¦‚æœæ²¡æœ‰ collectionï¼Œå°±åˆ›å»º
try:
    collection = client.get_collection("notes")
except:
    collection = client.create_collection("notes")

MODES = {
    "æ€»ç»“æ¨¡å¼ï¼ˆå¸¦æ€è€ƒé—®é¢˜ï¼‰": "1",
    "æé—®æ¨¡å¼": "2",
    "æ´å¯Ÿæ¨¡å¼ï¼ˆå¸¦è¡ŒåŠ¨æ­¥éª¤ï¼‰": "3"
}

# ----------------- å·¥å…·å‡½æ•° -----------------
def build_prompt(mode: str, context: str, query: str) -> str:
    if mode == "1":
        return f"""ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„ç¬”è®°ç‰‡æ®µï¼š
{context}

è¯·å¸®æˆ‘ç”¨ä¸­æ–‡æ€»ç»“è¿™äº›å†…å®¹ï¼Œæç‚¼å‡ºå…³é”®ç‚¹ã€‚"""
    elif mode == "2":
        return f"""ä½ æ˜¯ä¸€ä¸ªæœ¬åœ°çŸ¥è¯†åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„ç¬”è®°ç‰‡æ®µï¼š
{context}

åŸºäºè¿™äº›å†…å®¹ï¼Œå›ç­”æˆ‘çš„é—®é¢˜ï¼š{query}"""
    elif mode == "3":
        return f"""ä½ æ˜¯ä¸€ä¸ªæ´å¯ŸåŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„ç¬”è®°ç‰‡æ®µï¼š
{context}

è¯·å¸®æˆ‘æç‚¼å…³é”®è§è§£ï¼ŒæŒ‡å‡ºæ½œåœ¨è”ç³»ï¼Œç»™å‡ºæ–°çš„æ€è€ƒæ–¹å‘ã€‚"""
    else:
        return f"ä»¥ä¸‹æ˜¯æˆ‘çš„ç¬”è®°ç‰‡æ®µï¼š\n{context}\n\nåŸºäºè¿™äº›å†…å®¹ï¼Œå›ç­”é—®é¢˜ï¼š{query}"

def rag_query(query: str, mode: str):
    query_emb = embed(model=EMBEDDING_MODEL, input=query).embeddings[0]
    results = collection.query(query_embeddings=[query_emb], n_results=3)
    context = "\n\n".join(results["documents"][0]) if results["documents"] else "ï¼ˆæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼‰"
    prompt = build_prompt(mode, context, query)
    response = chat(model=MODEL_NAME, messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"], context

def generate_thinking_questions(summary: str):
    prompt = f"""åŸºäºä»¥ä¸‹æ€»ç»“å†…å®¹ï¼Œç”Ÿæˆ 3 ä¸ªä¸­æ–‡æ€è€ƒé—®é¢˜ï¼Œå¼•å¯¼æ·±å…¥ç†è§£å’Œåº”ç”¨ï¼š
{summary}"""
    response = chat(model=MODEL_NAME, messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

def generate_action_items(insights: str):
    prompt = f"""åŸºäºä»¥ä¸‹æ´å¯Ÿå†…å®¹ï¼Œæå‡º 3-5 ä¸ªå¯æ‰§è¡Œçš„è¡ŒåŠ¨æ­¥éª¤ï¼ˆaction itemsï¼‰ï¼Œå¸®åŠ©è½å®å’Œåº”ç”¨è¿™äº›æ´å¯Ÿï¼š
{insights}"""
    response = chat(model=MODEL_NAME, messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

def add_markdown_to_collection(file, filename: str):
    """è¯»å– markdown æ–‡ä»¶ï¼ŒæŒ‰æ®µè½åˆ‡åˆ†ï¼Œå­˜å…¥ Chroma"""
    text = file.read().decode("utf-8", errors="replace")
    chunks = text.split("\n\n")
    for i, chunk in enumerate(chunks):
        if chunk.strip():
            emb = embed(model=EMBEDDING_MODEL, input=chunk).embeddings[0]
            collection.add(
                documents=[chunk],
                embeddings=[emb],
                ids=[f"{filename}_{i}"]
            )


def clear_database():
    """æ¸…ç©ºå½“å‰æ•°æ®åº“"""
    global collection
    client.delete_collection("notes")
    collection = client.create_collection("notes")

# ----------------- Streamlit UI -----------------
st.set_page_config(page_title="RAG Chat", page_icon="ğŸ“š")

st.title("ğŸ“š æœ¬åœ° RAG å­¦ä¹ åŠ©æ‰‹")

# ---- æ–‡ä»¶ä¸Šä¼ åŒº ----
st.sidebar.header("ğŸ“‚ ä¸Šä¼  Markdown ç¬”è®°")
uploaded_files = st.sidebar.file_uploader("é€‰æ‹© .md æ–‡ä»¶", type=["md"], accept_multiple_files=True)

if uploaded_files:
    for f in uploaded_files:
        add_markdown_to_collection(f, f.name)
    st.sidebar.success(f"âœ… å·²æ·»åŠ  {len(uploaded_files)} ä¸ªç¬”è®°åˆ°æ•°æ®åº“")

# æ¸…ç©ºæ•°æ®åº“æŒ‰é’®
if st.sidebar.button("ğŸ—‘ï¸ æ¸…ç©ºæ•°æ®åº“"):
    clear_database()
    st.sidebar.warning("âš ï¸ æ•°æ®åº“å·²æ¸…ç©ºï¼")

# ---- æ¨¡å¼é€‰æ‹© ----
mode_label = st.radio("é€‰æ‹©æ¨¡å¼", list(MODES.keys()))
mode = MODES[mode_label]

query = st.text_area("è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆæ€»ç»“æ¨¡å¼å¯ç•™ç©ºï¼‰", "")

if st.button("è¿è¡Œ RAG"):
    if mode == "1":  # æ€»ç»“æ¨¡å¼
        # ç›´æ¥å–æ‰€æœ‰æ–‡æ¡£
        all_docs = collection.get()["documents"]
        context = "\n\n".join(all_docs) if all_docs else "ï¼ˆæ²¡æœ‰æ‰¾åˆ°ç¬”è®°å†…å®¹ï¼‰"

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„ç¬”è®°ç‰‡æ®µï¼š
    {context}

    è¯·å¸®æˆ‘ç”¨ä¸­æ–‡æ€»ç»“è¿™äº›å†…å®¹ï¼Œæç‚¼å‡ºå…³é”®ç‚¹ã€‚"""
        response = chat(model=MODEL_NAME, messages=[{"role": "user", "content": prompt}])
        st.session_state["summary_answer"] = response["message"]["content"]  # âœ… å­˜å…¥ session_state

        st.subheader("ğŸ“ æ€»ç»“")
        st.write(st.session_state["summary_answer"])

        st.subheader("ğŸ“‘ å‚è€ƒå†…å®¹")
        st.write(context)

    # å•ç‹¬çš„æŒ‰é’®ï¼Œç”¨ä¹‹å‰å­˜çš„ç»“æœ
    if mode == "1" and st.button("ğŸ§  ç”Ÿæˆæ€è€ƒé—®é¢˜"):
        if "summary_answer" in st.session_state:
            st.subheader("ğŸ§  æ€è€ƒé—®é¢˜")
            st.write(generate_thinking_questions(st.session_state["summary_answer"]))
        else:
            st.warning("è¯·å…ˆç‚¹å‡»è¿è¡Œ RAG ç”Ÿæˆæ€»ç»“ï¼")


    else:  # æé—®æ¨¡å¼ / æ´å¯Ÿæ¨¡å¼
        if query.strip():
            answer, context = rag_query(query, mode)
            st.subheader("ğŸ’¡ å›ç­”")
            st.write(answer)
            st.subheader("ğŸ“‘ å‚è€ƒå†…å®¹")
            st.write(context)
            if mode == "3":
                if st.button("ğŸš€ ç”Ÿæˆè¡ŒåŠ¨æ­¥éª¤"):
                    st.subheader("ğŸš€ è¡ŒåŠ¨æ­¥éª¤")
                    st.write(generate_action_items(answer))
        else:
            st.warning("è¯·è¾“å…¥é—®é¢˜ï¼")
